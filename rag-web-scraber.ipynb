{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/'}, page_content='How to load web pages\\nThis guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.\\nLangChain integrates with a host of parsers that are appropriate for web pages. The right parser will depend on your needs. Below we demonstrate two possibilities:\\n\\nSimple and fast parsing, in which we recover one Document per web page with its content represented as a \"flattened\" string;\\nAdvanced parsing, in which we recover multiple Document objects per page, allowing one to identify and traverse sections, links, tables, and other structures.\\n\\nSetup\\u200b\\nFor the \"simple and fast\" parsing, we will need langchain-community and the beautifulsoup4 library:\\n%pip install -qU langchain-community beautifulsoup4\\nFor advanced parsing, we will use langchain-unstructured:\\n%pip install -qU langchain-unstructured\\nSimple and fast text extraction\\u200b\\nIf you are looking for a simple string representation of text that is embedded in a web page, the method below is appropriate. It will return a list of Document objects -- one per page -- containing a single string of the page\\'s text. Under the hood it uses the beautifulsoup4 Python library.\\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\\nimport bs4from langchain_community.document_loaders import WebBaseLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = WebBaseLoader(web_paths=[page_url])docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]API Reference:WebBaseLoader\\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\\nprint(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500].strip())\\n{\\'source\\': \\'https://python.langchain.com/docs/how_to/chatbots_memory/\\', \\'title\\': \\'How to add memory to chatbots | \\\\uf8ffü¶úÔ∏è\\\\uf8ffüîó LangChain\\', \\'description\\': \\'A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\\', \\'language\\': \\'en\\'}How to add memory to chatbots | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChainSkip to main contentShare your thoughts on AI agents. Take the 3-min survey.IntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingd\\nThis is essentially a dump of the text from the page\\'s HTML. It may contain extraneous information like headings and navigation bars. If you are familiar with the expected HTML, you can specify desired <div> classes and other parameters via BeautifulSoup. Below we parse only the body text of the article:\\nloader = WebBaseLoader(    web_paths=[page_url],    bs_kwargs={        \"parse_only\": bs4.SoupStrainer(class_=\"theme-doc-markdown markdown\"),    },    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},)docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]\\nprint(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500])\\n{\\'source\\': \\'https://python.langchain.com/docs/how_to/chatbots_memory/\\'}How to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We\\'ll go into more detail on a few techniq\\nprint(doc.page_content[-500:])\\na greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.\\'), | HumanMessage(content=\\'What did I say my name was?\\'), | AIMessage(content=\\'You introduced yourself as Nemo. How can I assist you today, Nemo?\\')] | Note that invoking the chain again will generate another summary generated from the initial summary plus new messages and so on. You could also design a hybrid approach where a certain number of messages are retained in chat history while others are summarized.\\nNote that this required advance technical knowledge of how the body text is represented in the underlying HTML.\\nWe can parameterize WebBaseLoader with a variety of settings, allowing for specification of request headers, rate limits, and parsers and other kwargs for BeautifulSoup. See its API reference for detail.\\nAdvanced parsing\\u200b\\nThis method is appropriate if we want more granular control or processing of the page content. Below, instead of generating one Document per page and controlling its content via BeautifulSoup, we generate multiple Document objects representing distinct structures on a page. These structures can include section titles and their corresponding body texts, lists or enumerations, tables, and more.\\nUnder the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain.\\nfrom langchain_unstructured import UnstructuredLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = UnstructuredLoader(web_url=page_url)docs = []async for doc in loader.alazy_load():    docs.append(doc)API Reference:UnstructuredLoader\\nINFO: Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.INFO: NumExpr defaulting to 8 threads.\\nNote that with no advance knowledge of the page HTML structure, we recover a natural organization of the body text:\\nfor doc in docs[:5]:    print(doc.page_content)\\nHow to add memory to chatbotsA key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:Simply stuffing previous messages into a chat model prompt.The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.More complex modifications like synthesizing summaries for long running conversations.ERROR! Session/line number was not unique in database. History logging moved to new session 2747\\nExtracting content from specific sections\\u200b\\nEach Document object represents an element of the page. Its metadata contains useful information, such as its category:\\nfor doc in docs[:5]:    print(f\\'{doc.metadata[\"category\"]}: {doc.page_content}\\')\\nTitle: How to add memory to chatbotsNarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:ListItem: Simply stuffing previous messages into a chat model prompt.ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.ListItem: More complex modifications like synthesizing summaries for long running conversations.\\nElements may also have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding Document objects.\\nAs an example, below we load the content of the \"Setup\" sections for two web pages:\\nfrom typing import Listfrom langchain_core.documents import Documentasync def _get_setup_docs_from_url(url: str) -> List[Document]:    loader = UnstructuredLoader(web_url=url)    setup_docs = []    parent_id = -1    async for doc in loader.alazy_load():        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):            parent_id = doc.metadata[\"element_id\"]        if doc.metadata.get(\"parent_id\") == parent_id:            setup_docs.append(doc)    return setup_docspage_urls = [    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",]setup_docs = []for url in page_urls:    page_setup_docs = await _get_setup_docs_from_url(url)    setup_docs.extend(page_setup_docs)API Reference:Document\\nfrom collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\\\n\"dict(setup_text)\\n{\\'https://python.langchain.com/docs/how_to/chatbots_memory/\\': \"You\\'ll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\\\\n%pip install --upgrade --quiet langchain langchain-openai\\\\n\\\\n# Set env var OPENAI_API_KEY or load from a .env file:\\\\nimport dotenv\\\\n\\\\ndotenv.load_dotenv()\\\\n[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\\\\nYou should consider upgrading via the \\'/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip\\' command.[0m[33m\\\\n[0mNote: you may need to restart the kernel to use updated packages.\\\\n\", \\'https://python.langchain.com/docs/how_to/chatbots_tools/\\': \"For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\\\nYou\\'ll need to sign up for an account on the Tavily website, and install the following packages:\\\\n%pip install --upgrade --quiet langchain-community langchain-openai tavily-python\\\\n\\\\n# Set env var OPENAI_API_KEY or load from a .env file:\\\\nimport dotenv\\\\n\\\\ndotenv.load_dotenv()\\\\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.\\\\n\"}\\nVector search over page content\\u200b\\nOnce we have loaded the page contents into LangChain Document objects, we can index them (e.g., for a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice.\\n%pip install -qU langchain-openai\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())retrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)for doc in retrieved_docs:    print(f\\'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\\\n\\')API Reference:InMemoryVectorStore | OpenAIEmbeddings\\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"``````outputPage https://python.langchain.com/docs/how_to/chatbots_tools/: You\\'ll need to sign up for an account on the Tavily website, and install the following packages:Page https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\nOther web page loaders\\u200b\\nFor a list of available LangChain web page loaders, please see this table.')]\n"
     ]
    }
   ],
   "source": [
    "bs4_strainer= bs4.SoupStrainer(class_='theme-doc-markdown markdown') # inspect the html and find the class name of the content\n",
    "\n",
    "loader= WebBaseLoader(web_paths=['https://python.langchain.com/docs/how_to/document_loader_web/',],\n",
    "                    bs_kwargs={'parse_only':bs4_strainer}) # only parse the content with class 'theme-doc-markdown markdown'\n",
    "docs= loader.load()\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter= RecursiveCharacterTextSplitter(chunk_size=1300, chunk_overlap=100, add_start_index=True)\n",
    "all_splits= splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 0}, page_content='How to load web pages\\nThis guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.\\nLangChain integrates with a host of parsers that are appropriate for web pages. The right parser will depend on your needs. Below we demonstrate two possibilities:\\n\\nSimple and fast parsing, in which we recover one Document per web page with its content represented as a \"flattened\" string;\\nAdvanced parsing, in which we recover multiple Document objects per page, allowing one to identify and traverse sections, links, tables, and other structures.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 727}, page_content='Setup\\u200b\\nFor the \"simple and fast\" parsing, we will need langchain-community and the beautifulsoup4 library:\\n%pip install -qU langchain-community beautifulsoup4\\nFor advanced parsing, we will use langchain-unstructured:\\n%pip install -qU langchain-unstructured\\nSimple and fast text extraction\\u200b\\nIf you are looking for a simple string representation of text that is embedded in a web page, the method below is appropriate. It will return a list of Document objects -- one per page -- containing a single string of the page\\'s text. Under the hood it uses the beautifulsoup4 Python library.\\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\\nimport bs4from langchain_community.document_loaders import WebBaseLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = WebBaseLoader(web_paths=[page_url])docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]API Reference:WebBaseLoader\\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\\nprint(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500].strip())'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 1863}, page_content='print(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500].strip())\\n{\\'source\\': \\'https://python.langchain.com/docs/how_to/chatbots_memory/\\', \\'title\\': \\'How to add memory to chatbots | \\\\uf8ffü¶úÔ∏è\\\\uf8ffüîó LangChain\\', \\'description\\': \\'A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\\', \\'language\\': \\'en\\'}How to add memory to chatbots | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChainSkip to main contentShare your thoughts on AI agents. Take the 3-min survey.IntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingd\\nThis is essentially a dump of the text from the page\\'s HTML. It may contain extraneous information like headings and navigation bars. If you are familiar with the expected HTML, you can specify desired <div> classes and other parameters via BeautifulSoup. Below we parse only the body text of the article:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 3062}, page_content='loader = WebBaseLoader(    web_paths=[page_url],    bs_kwargs={        \"parse_only\": bs4.SoupStrainer(class_=\"theme-doc-markdown markdown\"),    },    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},)docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]\\nprint(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500])\\n{\\'source\\': \\'https://python.langchain.com/docs/how_to/chatbots_memory/\\'}How to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We\\'ll go into more detail on a few techniq\\nprint(doc.page_content[-500:])'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 3997}, page_content=\"print(doc.page_content[-500:])\\na greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.'), | HumanMessage(content='What did I say my name was?'), | AIMessage(content='You introduced yourself as Nemo. How can I assist you today, Nemo?')] | Note that invoking the chain again will generate another summary generated from the initial summary plus new messages and so on. You could also design a hybrid approach where a certain number of messages are retained in chat history while others are summarized.\\nNote that this required advance technical knowledge of how the body text is represented in the underlying HTML.\\nWe can parameterize WebBaseLoader with a variety of settings, allowing for specification of request headers, rate limits, and parsers and other kwargs for BeautifulSoup. See its API reference for detail.\\nAdvanced parsing\\u200b\\nThis method is appropriate if we want more granular control or processing of the page content. Below, instead of generating one Document per page and controlling its content via BeautifulSoup, we generate multiple Document objects representing distinct structures on a page. These structures can include section titles and their corresponding body texts, lists or enumerations, tables, and more.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 5259}, page_content='Under the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain.\\nfrom langchain_unstructured import UnstructuredLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = UnstructuredLoader(web_url=page_url)docs = []async for doc in loader.alazy_load():    docs.append(doc)API Reference:UnstructuredLoader\\nINFO: Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.INFO: NumExpr defaulting to 8 threads.\\nNote that with no advance knowledge of the page HTML structure, we recover a natural organization of the body text:\\nfor doc in docs[:5]:    print(doc.page_content)\\nHow to add memory to chatbotsA key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:Simply stuffing previous messages into a chat model prompt.The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.More complex modifications like synthesizing summaries for long running conversations.ERROR! Session/line number was not unique in database. History logging moved to new session 2747'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 6517}, page_content='Extracting content from specific sections\\u200b\\nEach Document object represents an element of the page. Its metadata contains useful information, such as its category:\\nfor doc in docs[:5]:    print(f\\'{doc.metadata[\"category\"]}: {doc.page_content}\\')\\nTitle: How to add memory to chatbotsNarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:ListItem: Simply stuffing previous messages into a chat model prompt.ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.ListItem: More complex modifications like synthesizing summaries for long running conversations.\\nElements may also have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding Document objects.\\nAs an example, below we load the content of the \"Setup\" sections for two web pages:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 7486}, page_content='As an example, below we load the content of the \"Setup\" sections for two web pages:\\nfrom typing import Listfrom langchain_core.documents import Documentasync def _get_setup_docs_from_url(url: str) -> List[Document]:    loader = UnstructuredLoader(web_url=url)    setup_docs = []    parent_id = -1    async for doc in loader.alazy_load():        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):            parent_id = doc.metadata[\"element_id\"]        if doc.metadata.get(\"parent_id\") == parent_id:            setup_docs.append(doc)    return setup_docspage_urls = [    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",]setup_docs = []for url in page_urls:    page_setup_docs = await _get_setup_docs_from_url(url)    setup_docs.extend(page_setup_docs)API Reference:Document\\nfrom collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\\\n\"dict(setup_text)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 8543}, page_content='{\\'https://python.langchain.com/docs/how_to/chatbots_memory/\\': \"You\\'ll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\\\\n%pip install --upgrade --quiet langchain langchain-openai\\\\n\\\\n# Set env var OPENAI_API_KEY or load from a .env file:\\\\nimport dotenv\\\\n\\\\ndotenv.load_dotenv()\\\\n[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\\\\nYou should consider upgrading via the \\'/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip\\' command.[0m[33m\\\\n[0mNote: you may need to restart the kernel to use updated packages.\\\\n\", \\'https://python.langchain.com/docs/how_to/chatbots_tools/\\': \"For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\\\nYou\\'ll need to sign up for an account on the Tavily website, and install the following packages:\\\\n%pip install --upgrade --quiet langchain-community langchain-openai tavily-python\\\\n\\\\n# Set env var OPENAI_API_KEY or load from a .env file:\\\\nimport dotenv\\\\n\\\\ndotenv.load_dotenv()\\\\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 9762}, page_content='will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.\\\\n\"}'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 9866}, page_content='Vector search over page content\\u200b\\nOnce we have loaded the page contents into LangChain Document objects, we can index them (e.g., for a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice.\\n%pip install -qU langchain-openai\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())retrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)for doc in retrieved_docs:    print(f\\'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\\\n\\')API Reference:InMemoryVectorStore | OpenAIEmbeddings'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 10689}, page_content='INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"``````outputPage https://python.langchain.com/docs/how_to/chatbots_tools/: You\\'ll need to sign up for an account on the Tavily website, and install the following packages:Page https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\nOther web page loaders\\u200b\\nFor a list of available LangChain web page loaders, please see this table.')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embedding= OllamaEmbeddings(model='all-minilm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "vector_db= Chroma.from_documents(documents=all_splits, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='116d60a1-082d-4caf-9994-272b58747a01', metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 7486}, page_content='As an example, below we load the content of the \"Setup\" sections for two web pages:\\nfrom typing import Listfrom langchain_core.documents import Documentasync def _get_setup_docs_from_url(url: str) -> List[Document]:    loader = UnstructuredLoader(web_url=url)    setup_docs = []    parent_id = -1    async for doc in loader.alazy_load():        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):            parent_id = doc.metadata[\"element_id\"]        if doc.metadata.get(\"parent_id\") == parent_id:            setup_docs.append(doc)    return setup_docspage_urls = [    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",]setup_docs = []for url in page_urls:    page_setup_docs = await _get_setup_docs_from_url(url)    setup_docs.extend(page_setup_docs)API Reference:Document\\nfrom collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\\\n\"dict(setup_text)'),\n",
       " Document(id='a9f68c4c-84e4-4e3d-bdc9-8c9cfbd4b5dd', metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 7486}, page_content='As an example, below we load the content of the \"Setup\" sections for two web pages:\\nfrom typing import Listfrom langchain_core.documents import Documentasync def _get_setup_docs_from_url(url: str) -> List[Document]:    loader = UnstructuredLoader(web_url=url)    setup_docs = []    parent_id = -1    async for doc in loader.alazy_load():        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):            parent_id = doc.metadata[\"element_id\"]        if doc.metadata.get(\"parent_id\") == parent_id:            setup_docs.append(doc)    return setup_docspage_urls = [    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",]setup_docs = []for url in page_urls:    page_setup_docs = await _get_setup_docs_from_url(url)    setup_docs.extend(page_setup_docs)API Reference:Document\\nfrom collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\\\n\"dict(setup_text)'),\n",
       " Document(id='d8dcfedd-fa49-4686-8951-1bb372f13fca', metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'start_index': 727}, page_content='Setup\\u200b\\nFor the \"simple and fast\" parsing, we will need langchain-community and the beautifulsoup4 library:\\n%pip install -qU langchain-community beautifulsoup4\\nFor advanced parsing, we will use langchain-unstructured:\\n%pip install -qU langchain-unstructured\\nSimple and fast text extraction\\u200b\\nIf you are looking for a simple string representation of text that is embedded in a web page, the method below is appropriate. It will return a list of Document objects -- one per page -- containing a single string of the page\\'s text. Under the hood it uses the beautifulsoup4 Python library.\\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\\nimport bs4from langchain_community.document_loaders import WebBaseLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = WebBaseLoader(web_paths=[page_url])docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]API Reference:WebBaseLoader\\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\\nprint(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500].strip())')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever= vector_db.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "query= 'how to use web document loader in python'\n",
    "retrieved_docs= retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "context= ''.join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an example, below we load the content of the \"Setup\" sections for two web pages:\\nfrom typing import Listfrom langchain_core.documents import Documentasync def _get_setup_docs_from_url(url: str) -> List[Document]:    loader = UnstructuredLoader(web_url=url)    setup_docs = []    parent_id = -1    async for doc in loader.alazy_load():        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):            parent_id = doc.metadata[\"element_id\"]        if doc.metadata.get(\"parent_id\") == parent_id:            setup_docs.append(doc)    return setup_docspage_urls = [    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",]setup_docs = []for url in page_urls:    page_setup_docs = await _get_setup_docs_from_url(url)    setup_docs.extend(page_setup_docs)API Reference:Document\\nfrom collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\\\n\"dict(setup_text)As an example, below we load the content of the \"Setup\" sections for two web pages:\\nfrom typing import Listfrom langchain_core.documents import Documentasync def _get_setup_docs_from_url(url: str) -> List[Document]:    loader = UnstructuredLoader(web_url=url)    setup_docs = []    parent_id = -1    async for doc in loader.alazy_load():        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):            parent_id = doc.metadata[\"element_id\"]        if doc.metadata.get(\"parent_id\") == parent_id:            setup_docs.append(doc)    return setup_docspage_urls = [    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",]setup_docs = []for url in page_urls:    page_setup_docs = await _get_setup_docs_from_url(url)    setup_docs.extend(page_setup_docs)API Reference:Document\\nfrom collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\\\n\"dict(setup_text)Setup\\u200b\\nFor the \"simple and fast\" parsing, we will need langchain-community and the beautifulsoup4 library:\\n%pip install -qU langchain-community beautifulsoup4\\nFor advanced parsing, we will use langchain-unstructured:\\n%pip install -qU langchain-unstructured\\nSimple and fast text extraction\\u200b\\nIf you are looking for a simple string representation of text that is embedded in a web page, the method below is appropriate. It will return a list of Document objects -- one per page -- containing a single string of the page\\'s text. Under the hood it uses the beautifulsoup4 Python library.\\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\\nimport bs4from langchain_community.document_loaders import WebBaseLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = WebBaseLoader(web_paths=[page_url])docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]API Reference:WebBaseLoader\\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\\nprint(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500].strip())'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To use the WebDocumentLoader in Python context for \"simple and fast\" parsing:\\n\\n1. Install langchain-community and beautifulsoup4 if you haven\\'t already:\\n   ```bash\\npip install -qU langchain-community beautifulsoup4\\n```\\n\\n2. Import required libraries and adjust the `WebBaseLoader` to suit your needs.\\n\\n3. Load the web pages using the loader\\'s `alazy_load()` method, specifying the URL of each page:\\n\\n```python\\nimport bs4\\n\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom typing import List\\n\\n# Adjust the page URLs in \\'page_urls\\' list according to your needs\\n\\ndef _get_setup_docs_from_url(url: str) -> List[Document]:\\n    loader = WebBaseLoader(web_paths=[url])\\n    setup_docs = []\\n    parent_id = -1\\n    async for doc in loader.alazy_load():\\n        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):\\n            parent_id = doc.metadata[\"element_id\"]\\n        if doc.metadata.get(\"parent_id\") == parent_id:\\n            setup_docs.append(doc)\\n    return setup_docs\\n\\npage_urls = [\"https://python.langchain.com/docs/how_to/chatbots_memory/\"]\\n\\nsetup_docs = []\\nfor url in page_urls:\\n    page_setup_docs = await _get_setup_docs_from_url(url)\\n    setup_docs.extend(page_setup_docs)\\n\\n# Simple and fast text extraction\\nimport bs4\\n\\ndocs = []  # Initialize an empty list to store the documents\\nasync for doc in loader.alazy_load():\\n    docs.append(doc)  # Add each document from the loader\\'s async iteration\\n    \\nassert len(docs) == 1  # Check if exactly one document is loaded\\n    \\ndoc = docs[0]  # Access and print the first document\\nprint(f\"Page {doc.metadata[\\'page_title\\']} ({doc.metadata[\\'url\\']})\")\\nprint(doc.page_content[:500].strip())\\n```'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "llm= OllamaLLM(model='llama3.2:1b')\n",
    "\n",
    "prompt= f\"\"\"answer the question according to the context briefly: question:{query} context:{context})\"\"\"\n",
    "\n",
    "response= llm.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To use the WebDocumentLoader in Python context for \"simple and fast\" parsing:\n",
       "\n",
       "1. Install langchain-community and beautifulsoup4 if you haven't already:\n",
       "   ```bash\n",
       "pip install -qU langchain-community beautifulsoup4\n",
       "```\n",
       "\n",
       "2. Import required libraries and adjust the `WebBaseLoader` to suit your needs.\n",
       "\n",
       "3. Load the web pages using the loader's `alazy_load()` method, specifying the URL of each page:\n",
       "\n",
       "```python\n",
       "import bs4\n",
       "\n",
       "from langchain_community.document_loaders import WebBaseLoader\n",
       "from typing import List\n",
       "\n",
       "# Adjust the page URLs in 'page_urls' list according to your needs\n",
       "\n",
       "def _get_setup_docs_from_url(url: str) -> List[Document]:\n",
       "    loader = WebBaseLoader(web_paths=[url])\n",
       "    setup_docs = []\n",
       "    parent_id = -1\n",
       "    async for doc in loader.alazy_load():\n",
       "        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):\n",
       "            parent_id = doc.metadata[\"element_id\"]\n",
       "        if doc.metadata.get(\"parent_id\") == parent_id:\n",
       "            setup_docs.append(doc)\n",
       "    return setup_docs\n",
       "\n",
       "page_urls = [\"https://python.langchain.com/docs/how_to/chatbots_memory/\"]\n",
       "\n",
       "setup_docs = []\n",
       "for url in page_urls:\n",
       "    page_setup_docs = await _get_setup_docs_from_url(url)\n",
       "    setup_docs.extend(page_setup_docs)\n",
       "\n",
       "# Simple and fast text extraction\n",
       "import bs4\n",
       "\n",
       "docs = []  # Initialize an empty list to store the documents\n",
       "async for doc in loader.alazy_load():\n",
       "    docs.append(doc)  # Add each document from the loader's async iteration\n",
       "    \n",
       "assert len(docs) == 1  # Check if exactly one document is loaded\n",
       "    \n",
       "doc = docs[0]  # Access and print the first document\n",
       "print(f\"Page {doc.metadata['page_title']} ({doc.metadata['url']})\")\n",
       "print(doc.page_content[:500].strip())\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
